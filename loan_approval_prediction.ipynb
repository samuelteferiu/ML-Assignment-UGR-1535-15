{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¹ Loan Approval Prediction â­â­â­â­â­\n",
        "\n",
        "## Predict: Loan Approved (Yes/No)\n",
        "\n",
        "### Features:\n",
        "- **income**: Applicant's annual income\n",
        "- **credit_score**: Credit score of the applicant\n",
        "- **employment_years**: Years of employment\n",
        "- **loan_amount**: Requested loan amount\n",
        "\n",
        "### Models:\n",
        "1. **Logistic Regression**\n",
        "2. **Decision Tree**\n",
        "\n",
        "### Pipeline:\n",
        "1. Data Loading & Exploration\n",
        "2. Data Cleaning & Preprocessing\n",
        "3. Feature Engineering\n",
        "4. Model Training & Evaluation\n",
        "5. Model Export (joblib)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic loan approval dataset\n",
        "# In a real scenario, you would load from Kaggle or other sources\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 10000\n",
        "\n",
        "# Generate features\n",
        "income = np.random.normal(50000, 20000, n_samples)\n",
        "income = np.clip(income, 20000, 150000)  # Clip to reasonable range\n",
        "\n",
        "credit_score = np.random.normal(650, 100, n_samples)\n",
        "credit_score = np.clip(credit_score, 300, 850)\n",
        "\n",
        "employment_years = np.random.exponential(5, n_samples)\n",
        "employment_years = np.clip(employment_years, 0, 40)\n",
        "\n",
        "loan_amount = np.random.normal(30000, 20000, n_samples)\n",
        "loan_amount = np.clip(loan_amount, 5000, 200000)\n",
        "\n",
        "# Create loan-to-income ratio (important feature)\n",
        "loan_to_income = loan_amount / income\n",
        "\n",
        "# Generate target variable (loan_approved) based on business logic\n",
        "# Higher income, credit score, employment years = higher approval chance\n",
        "# Lower loan amount relative to income = higher approval chance\n",
        "approval_prob = (\n",
        "    0.3 * (income / 100000) +  # Income factor\n",
        "    0.3 * (credit_score / 850) +  # Credit score factor\n",
        "    0.2 * (employment_years / 20) +  # Employment stability\n",
        "    0.2 * (1 - np.clip(loan_to_income, 0, 1))  # Loan-to-income ratio (inverse)\n",
        ")\n",
        "\n",
        "# Add some noise\n",
        "approval_prob += np.random.normal(0, 0.1, n_samples)\n",
        "approval_prob = np.clip(approval_prob, 0, 1)\n",
        "\n",
        "# Convert to binary\n",
        "loan_approved = (approval_prob > 0.5).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'income': income,\n",
        "    'credit_score': credit_score,\n",
        "    'employment_years': employment_years,\n",
        "    'loan_amount': loan_amount,\n",
        "    'loan_approved': loan_approved\n",
        "})\n",
        "\n",
        "print(\"Dataset created successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"=\" * 50)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"\\nColumn Names: {df.columns.tolist()}\")\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "print(f\"\\nBasic Statistics:\\n{df.describe()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check target variable distribution\n",
        "print(\"=\" * 50)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nLoan Approval Distribution:\\n{df['loan_approved'].value_counts()}\")\n",
        "print(f\"\\nLoan Approval Percentage:\\n{df['loan_approved'].value_counts(normalize=True) * 100}\")\n",
        "\n",
        "# Visualize target distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['loan_approved'].value_counts().plot(kind='bar', color=['#ff6b6b', '#51cf66'])\n",
        "plt.title('Loan Approval Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Loan Approved (0=No, 1=Yes)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Feature Distributions', fontsize=16, fontweight='bold')\n",
        "\n",
        "features = ['income', 'credit_score', 'employment_years', 'loan_amount']\n",
        "colors = ['#4dabf7', '#51cf66', '#ffd43b', '#ff6b6b']\n",
        "\n",
        "for idx, (feature, color) in enumerate(zip(features, colors)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    df[feature].hist(bins=50, ax=ax, color=color, alpha=0.7, edgecolor='black')\n",
        "    ax.set_title(f'{feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel(feature.replace(\"_\", \" \").title(), fontsize=10)\n",
        "    ax.set_ylabel('Frequency', fontsize=10)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots to check for outliers\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Feature Distributions by Loan Approval Status', fontsize=16, fontweight='bold')\n",
        "\n",
        "features = ['income', 'credit_score', 'employment_years', 'loan_amount']\n",
        "colors = ['#4dabf7', '#51cf66', '#ffd43b', '#ff6b6b']\n",
        "\n",
        "for idx, (feature, color) in enumerate(zip(features, colors)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    df.boxplot(column=feature, by='loan_approved', ax=ax, patch_artist=True,\n",
        "               boxprops=dict(facecolor=color, alpha=0.7))\n",
        "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} by Approval Status', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Loan Approved (0=No, 1=Yes)', fontsize=10)\n",
        "    ax.set_ylabel(feature.replace(\"_\", \" \").title(), fontsize=10)\n",
        "    plt.suptitle('')  # Remove default title\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning & Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values check:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Check for duplicates\n",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Check for negative values in features that shouldn't have them\n",
        "print(\"\\nNegative values check:\")\n",
        "print(f\"Income: {(df['income'] < 0).sum()}\")\n",
        "print(f\"Credit Score: {(df['credit_score'] < 0).sum()}\")\n",
        "print(f\"Employment Years: {(df['employment_years'] < 0).sum()}\")\n",
        "print(f\"Loan Amount: {(df['loan_amount'] < 0).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle any outliers using IQR method (optional, but good practice)\n",
        "def remove_outliers_iqr(df, columns):\n",
        "    df_clean = df.copy()\n",
        "    for col in columns:\n",
        "        Q1 = df_clean[col].quantile(0.25)\n",
        "        Q3 = df_clean[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "    return df_clean\n",
        "\n",
        "# For this dataset, we'll keep all data as it's synthetic and already bounded\n",
        "# But in real scenarios, you might want to remove outliers\n",
        "print(\"Data cleaning complete. All values are within reasonable ranges.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Create additional features\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Loan to Income Ratio (important feature)\n",
        "df_processed['loan_to_income_ratio'] = df_processed['loan_amount'] / df_processed['income']\n",
        "\n",
        "# Debt Service Ratio (monthly loan payment / monthly income)\n",
        "# Assuming 5% annual interest rate and 5-year term\n",
        "monthly_interest_rate = 0.05 / 12\n",
        "num_payments = 60  # 5 years\n",
        "monthly_loan_payment = df_processed['loan_amount'] * (\n",
        "    monthly_interest_rate * (1 + monthly_interest_rate)**num_payments\n",
        ") / ((1 + monthly_interest_rate)**num_payments - 1)\n",
        "monthly_income = df_processed['income'] / 12\n",
        "df_processed['debt_service_ratio'] = monthly_loan_payment / monthly_income\n",
        "\n",
        "# Credit Score Category (optional, can help with non-linear relationships)\n",
        "df_processed['credit_score_category'] = pd.cut(\n",
        "    df_processed['credit_score'],\n",
        "    bins=[0, 580, 670, 740, 850],\n",
        "    labels=['Poor', 'Fair', 'Good', 'Excellent']\n",
        ")\n",
        "\n",
        "print(\"Feature engineering complete!\")\n",
        "print(f\"\\nNew features created:\")\n",
        "print(f\"- loan_to_income_ratio\")\n",
        "print(f\"- debt_service_ratio\")\n",
        "print(f\"- credit_score_category\")\n",
        "print(f\"\\nUpdated shape: {df_processed.shape}\")\n",
        "df_processed.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for modeling\n",
        "# Select features for modeling\n",
        "feature_columns = ['income', 'credit_score', 'employment_years', 'loan_amount', \n",
        "                   'loan_to_income_ratio', 'debt_service_ratio']\n",
        "\n",
        "# One-hot encode credit_score_category if using it\n",
        "# For simplicity, we'll use numeric features only\n",
        "X = df_processed[feature_columns]\n",
        "y = df_processed['loan_approved']\n",
        "\n",
        "print(\"Features selected:\")\n",
        "print(feature_columns)\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Data split complete!\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nTest set target distribution:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training\n",
        "\n",
        "### 4.1 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline with StandardScaler and Logistic Regression\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "y_pred_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOGISTIC REGRESSION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred_lr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix for Logistic Regression\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Not Approved', 'Approved'],\n",
        "            yticklabels=['Not Approved', 'Approved'])\n",
        "plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve for Logistic Regression\n",
        "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_proba_lr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_pred_proba_lr):.4f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Logistic Regression', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline with StandardScaler and Decision Tree\n",
        "# Note: Decision Tree doesn't require scaling, but we'll keep it for consistency\n",
        "dt_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Decision Tree model...\")\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_pipeline.predict(X_test)\n",
        "y_pred_proba_dt = dt_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DECISION TREE RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_dt):.4f}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred_dt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix for Decision Tree\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens', cbar=True,\n",
        "            xticklabels=['Not Approved', 'Approved'],\n",
        "            yticklabels=['Not Approved', 'Approved'])\n",
        "plt.title('Decision Tree - Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve for Decision Tree\n",
        "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_score(y_test, y_pred_proba_dt):.4f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Decision Tree', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance for Decision Tree\n",
        "feature_importance = dt_pipeline.named_steps['classifier'].feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')\n",
        "plt.title('Decision Tree - Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(importance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare both models\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, y_pred_lr),\n",
        "        accuracy_score(y_test, y_pred_dt)\n",
        "    ],\n",
        "    'ROC-AUC': [\n",
        "        roc_auc_score(y_test, y_pred_proba_lr),\n",
        "        roc_auc_score(y_test, y_pred_proba_dt)\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
        "            color=['#4dabf7', '#51cf66'], alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# ROC-AUC comparison\n",
        "axes[1].bar(comparison_df['Model'], comparison_df['ROC-AUC'], \n",
        "            color=['#4dabf7', '#51cf66'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Model ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('ROC-AUC Score', fontsize=11)\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['ROC-AUC']):\n",
        "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combined ROC Curve\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_pred_proba_lr):.4f})', \n",
        "         linewidth=2, color='#4dabf7')\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_score(y_test, y_pred_proba_dt):.4f})', \n",
        "         linewidth=2, color='#51cf66')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1.5)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export both models using joblib\n",
        "import os\n",
        "\n",
        "# Create model directory if it doesn't exist\n",
        "os.makedirs('Backend/model', exist_ok=True)\n",
        "\n",
        "# Export Logistic Regression model\n",
        "joblib.dump(lr_pipeline, 'Backend/model/loan_approval_lr.pkl')\n",
        "print(\"âœ“ Logistic Regression model exported to 'Backend/model/loan_approval_lr.pkl'\")\n",
        "\n",
        "# Export Decision Tree model\n",
        "joblib.dump(dt_pipeline, 'Backend/model/loan_approval_dt.pkl')\n",
        "print(\"âœ“ Decision Tree model exported to 'Backend/model/loan_approval_dt.pkl'\")\n",
        "\n",
        "# Also export feature names for reference\n",
        "import json\n",
        "model_info = {\n",
        "    'feature_names': feature_columns,\n",
        "    'target_name': 'loan_approved',\n",
        "    'model_types': ['logistic_regression', 'decision_tree']\n",
        "}\n",
        "\n",
        "with open('Backend/model/model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "print(\"âœ“ Model info exported to 'Backend/model/model_info.json'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL EXPORT COMPLETE!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the exported models can be loaded\n",
        "print(\"Verifying exported models...\")\n",
        "\n",
        "# Load and test Logistic Regression\n",
        "lr_loaded = joblib.load('Backend/model/loan_approval_lr.pkl')\n",
        "test_pred_lr = lr_loaded.predict(X_test[:5])\n",
        "print(f\"âœ“ Logistic Regression model loaded successfully\")\n",
        "print(f\"  Sample predictions: {test_pred_lr}\")\n",
        "\n",
        "# Load and test Decision Tree\n",
        "dt_loaded = joblib.load('Backend/model/loan_approval_dt.pkl')\n",
        "test_pred_dt = dt_loaded.predict(X_test[:5])\n",
        "print(f\"âœ“ Decision Tree model loaded successfully\")\n",
        "print(f\"  Sample predictions: {test_pred_dt}\")\n",
        "\n",
        "print(\"\\nAll models exported and verified successfully! âœ…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "### Model Performance Summary:\n",
        "- **Logistic Regression**: Good interpretability, linear relationships\n",
        "- **Decision Tree**: Non-linear relationships, feature importance insights\n",
        "\n",
        "### Next Steps:\n",
        "1. Models are exported and ready for deployment\n",
        "2. Integrate models into FastAPI backend\n",
        "3. Create frontend interface for loan approval prediction\n",
        "4. Deploy application to production\n",
        "\n",
        "### Files Created:\n",
        "- `Backend/model/loan_approval_lr.pkl` - Logistic Regression model\n",
        "- `Backend/model/loan_approval_dt.pkl` - Decision Tree model\n",
        "- `Backend/model/model_info.json` - Model metadata\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
